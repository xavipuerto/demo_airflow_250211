Weather Data Pipeline - Airflow & TimescaleDB

Este proyecto implementa una ETL para obtener datos meteorolÃ³gicos de una API pÃºblica y almacenarlos en una base de datos TimescaleDB usando Airflow. Se orquestan tres tareas principales:

Obtener datos meteorolÃ³gicos para Madrid y ParÃ­s desde open-meteo.com.

Guardar los datos en tablas separadas (weather_madrid y weather_paris).

Consolidar los datos en la tabla weather_consolidated.

ğŸ“Œ Estructura del Proyecto

ğŸ“¦ demo_airflow_250211
â”œâ”€â”€ ğŸ“‚ dags/                # DAGs de Airflow
â”‚   â”œâ”€â”€ CA_meteo.py
â”‚   â”œâ”€â”€ CA_meteo_2.py
â”‚   â”œâ”€â”€ ca_meteo3.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ğŸ“‚ script/              # Scripts auxiliares en Python
â”‚   â”œâ”€â”€ fetch_weather.py
â”‚   â”œâ”€â”€ store_weather.py
â”‚   â”œâ”€â”€ consolidate_weather.py
â”‚   â””â”€â”€ store_weather_data.py
â”œâ”€â”€ ğŸ“„ docker-compose.yml    # ConfiguraciÃ³n de Docker para levantar Airflow
â”œâ”€â”€ ğŸ“„ .gitignore           # Archivos y carpetas ignorados por Git
â””â”€â”€ ğŸ“„ readme.md            # DocumentaciÃ³n del proyecto


âš™ï¸ ConfiguraciÃ³n del Entorno

ğŸ“‚ CreaciÃ³n de directorios

Ejecutar los siguientes comandos para asegurarse de que los directorios existen:

sudo mkdir -p /opt/emasesa/{dags,script,logs}

ğŸ“¦ InstalaciÃ³n de dependencias

AsegÃºrate de que tienes Docker y docker-compose instalados.
Instala las librerÃ­as necesarias dentro del contenedor Airflow:

pip install apache-airflow sqlalchemy psycopg2 requests

ğŸš€ Levantar los servicios

El entorno se levanta con Docker Compose. Usa:

cd /opt/emasesa/
docker-compose up -d

Esto iniciarÃ¡ los contenedores de Airflow, PostgreSQL y TimescaleDB.

Accede a la UI de Airflow en:

http://localhost:8080
Usuario: admin
ContraseÃ±a: admin

ğŸ”Œ ConexiÃ³n a la Base de Datos

Puedes conectarte a la base de datos TimescaleDB desde un contenedor con:

docker exec -it emasesa-timescaledb-1 psql -U postgres -d postgres

O desde un cliente externo:

Host: localhost
Puerto: 5433
Usuario: postgres
ContraseÃ±a: password
Base de datos: postgres

ğŸ“Š CreaciÃ³n del Esquema y Tablas

Para inicializar el esquema de datos en TimescaleDB, ejecuta:

CREATE SCHEMA IF NOT EXISTS public_api;

CREATE TABLE public_api.weather_madrid (
    id SERIAL PRIMARY KEY,
    ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    api_url TEXT NOT NULL,
    data JSONB NOT NULL,
    received_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);


CREATE TABLE public_api.weather_paris (
    id SERIAL PRIMARY KEY,
    ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    api_url TEXT NOT NULL,
    data JSONB NOT NULL,
    received_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE TABLE public_api.weather_consolidated (
    id SERIAL PRIMARY KEY,
    ts TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    location TEXT NOT NULL,
    api_url TEXT NOT NULL,
    data JSONB NOT NULL,
    received_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

ğŸ”„ Flujo de EjecuciÃ³n

El DAG de Airflow CA_meteo.py y CA_meteo_2.py gestionan la ejecuciÃ³n:

CA_meteo.py (Usa PythonOperator para llamada a API y almacenamiento directo en BD)

# Se ejecutan en Airflow
fetch_and_store_madrid
fetch_and_store_paris

CA_meteo_2.py (Usa BashOperator con scripts Python)

1ï¸âƒ£ Descarga datos de la API:

python /opt/emasesa/script/fetch_weather.py madrid > /tmp/weather_madrid.json
python /opt/emasesa/script/fetch_weather.py paris > /tmp/weather_paris.json

2ï¸âƒ£ Almacena los datos en TimescaleDB:

python /opt/emasesa/script/store_weather.py /tmp/weather_madrid.json
python /opt/emasesa/script/store_weather.py /tmp/weather_paris.json

3ï¸âƒ£ Consolida los datos en una tabla final:

python /opt/emasesa/script/consolidate_weather.py

ğŸ“¡ Consultas SQL

Ver los datos almacenados:

SELECT * FROM public_api.weather_madrid;
SELECT * FROM public_api.weather_paris;
SELECT * FROM public_api.weather_consolidated;

ğŸ› ï¸ DepuraciÃ³n y Logs

Si un proceso falla, revisa los logs en /opt/emasesa/logs/:

cat /opt/emasesa/logs/fetch_weather.log
cat /opt/emasesa/logs/store_weather.log
cat /opt/emasesa/logs/consolidate_weather.log

O en la interfaz de Airflow en la pestaÃ±a Logs.

ğŸ“Œ Notas Finales

Los DAGs estÃ¡n programados para ejecutarse cada hora.

Si quieres forzar la ejecuciÃ³n, usa:

airflow dags trigger get_data_meteo
airflow dags trigger weather_data_pipeline_emasesa

Si quieres parar y eliminar los contenedores:

docker-compose down --volumes

ğŸš€ Ahora el pipeline estÃ¡ listo para usarse! ğŸ¯
